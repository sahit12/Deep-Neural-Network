{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow are array manipulation libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### They are basically functions on Tensors(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(5)\n",
    "x2 = tf.constant(6)\n",
    "result = tf.multiply(x1, x2)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(result)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are working with MNIST dataset : It has 60,000 training samples and 10,000 testing samples of hand-written and labelled digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Working\n",
    "\n",
    "### First : we take the input from dataset and send it to hidden layer 1, where it will go activation function(so that neuron can decide to fire or not to the output layer or some other hidden layer).\n",
    "\n",
    "### We will have three hidden layers in this example\n",
    "\n",
    "### From the output we will get we will compare the output to the intended output. We will use a cost function(aslo called loss function), to determine how wrong we are.\n",
    "\n",
    "### Finally, we will use 'Adam Optimizer' to minimize the cost.\n",
    "\n",
    "#### Note: The slower the learning rate the better the results. Also, the act of sending the data through our network means we are using a feed forward neural network. The adjusting of weights backward is called back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IAMLEGEND\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-daad9455aabf>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\IAMLEGEND\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\IAMLEGEND\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\IAMLEGEND\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\IAMLEGEND\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\IAMLEGEND\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 1000\n",
    "n_nodes_hl2 = 1000\n",
    "n_nodes_hl3 = 1000\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "# We begin by specifying how many nodes each hidden layer will have, how many classes our dataset has, what our batch size is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "# These are our placeholders for some value in our graph \n",
    "# We are complete with constants and starting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now can make Neural Network:\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    # Here we are defining our weights and biases. The bias is a value which is added to our sum before being passed through activation function\n",
    "    # Now lets start the flow\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l1, output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-3ac9f7b62d07>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch 0 Completed out of 10 loss: 14779.510487794876\n",
      "Epoch 1 Completed out of 10 loss: 3798.066545009613\n",
      "Epoch 2 Completed out of 10 loss: 2370.023907490133\n",
      "Epoch 3 Completed out of 10 loss: 1609.928365690444\n",
      "Epoch 4 Completed out of 10 loss: 1167.5937127486238\n",
      "Epoch 5 Completed out of 10 loss: 848.6136923372871\n",
      "Epoch 6 Completed out of 10 loss: 620.3737804857237\n",
      "Epoch 7 Completed out of 10 loss: 451.02020531488586\n",
      "Epoch 8 Completed out of 10 loss: 322.0299823098754\n",
      "Epoch 9 Completed out of 10 loss: 238.4762162873499\n",
      "Accuracy: 0.9487\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y:epoch_y})\n",
    "                epoch_loss +=c\n",
    "                \n",
    "            print('Epoch', epoch, 'Completed out of', hm_epochs, 'loss:', epoch_loss)\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:' ,accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "        \n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
